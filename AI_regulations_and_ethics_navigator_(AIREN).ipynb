{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZu/JPYiUglF6PxUz/BoeL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmailman/AI_regulations_and_ethics_navigator_-AIREN-/blob/main/AI_regulations_and_ethics_navigator_(AIREN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AI regulations and ethics navigator (AIREN)**\n",
        "\n",
        "Joshua Banks Mailman, March 2024\n"
      ],
      "metadata": {
        "id": "o5JiRlv4_xA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a prototype retrieval based chatbot that focuses on AI ethics and regulation, in two senses:\n",
        "\n",
        "(1) It enables developers to makes queries about current or potential AI capabilities and receive responses that are informed by scholarly documents on the ethics and morality of AI as well as the November 2023 White House Executive Order on AI and the March 2024 EU AI Act.\n",
        "\n",
        "(2) It enables one to directly compare the White House Executive Order on AI and the EU AI Act with respect to any question, such as particular features to be added to an AI system"
      ],
      "metadata": {
        "id": "dch-fxsqBCu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools/packages used\n",
        "\n",
        "`LangChain`\n",
        "\n",
        "`ChromaDB`\n",
        "\n",
        "`OpenAI`\n",
        "\n",
        "The original documents as well as the vector store embeddings are kept on a Google drive."
      ],
      "metadata": {
        "id": "yLbmeDey_3wM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "tgd9aAMtD9qo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "KIsZd1NoDlRi"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import pypdf\n",
        "except:\n",
        "  !pip install pypdf\n",
        "  import pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import chromadb\n",
        "except:\n",
        "  !pip install chromadb\n",
        "  import chromadb"
      ],
      "metadata": {
        "id": "JNGWZXWtFXBk"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import tiktoken\n",
        "except:\n",
        "  !pip install tiktoken\n",
        "  import tiktoken"
      ],
      "metadata": {
        "id": "d9LR4FUNEteN"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3eaWVBSpDhH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import langchain\n",
        "except:\n",
        "  !pip install langchain\n",
        "  import langchain"
      ],
      "metadata": {
        "id": "wWSJ4nWRFiq9"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  #!pip install openai langchain -q\n",
        "  !pip install -U langchain-openai\n"
      ],
      "metadata": {
        "id": "XoHhbhy5F9Nj"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google drive for persistence"
      ],
      "metadata": {
        "id": "we6T9QWPBcdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "try:\n",
        "  drive.flush_and_unmount()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GEZ5iW7Ev-c",
        "outputId": "775f7607-cd68-48fe-cd56-129455bca228"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Folder structures"
      ],
      "metadata": {
        "id": "HwY1CTPRYw0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_folder = 'documents'\n",
        "emb_folder = 'embeddings'"
      ],
      "metadata": {
        "id": "poRytElME_ii"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "us_docs = 'us_regulations_etc'\n",
        "eu_docs = 'eu_regulations_etc'"
      ],
      "metadata": {
        "id": "7Hb8m6CMGYjG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6yeL55_3FHgl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_data_folder = '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation'\n",
        "gdrive_data_docs = gdrive_data_folder + '/' + doc_folder\n",
        "gdrive_data_embs = gdrive_data_folder + '/' + emb_folder\n"
      ],
      "metadata": {
        "id": "VBz82VErD7ji"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Curated list of relevant articles from the _Stanford Encyclopedia of Philosophy_"
      ],
      "metadata": {
        "id": "u95B4ybZCAIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sep_articles = [\n",
        "'logic-ai',\n",
        "'ethics-ai',\n",
        "'artificial-intelligence',\n",
        "'ethics-it-phenomenology',\n",
        "'it-privacy',\n",
        "'reasoning-automated',\n",
        "'ethics-business',\n",
        "'ethics-search',\n",
        "'ethics-social-networking',\n",
        "'ethics-manipulation',\n",
        "'it-moral-values',\n",
        "'computing-responsibility']\n",
        "\n",
        "\n",
        "# privacy-medicine/\n",
        "# computational-linguistics/\n",
        "# computational-philosophy/\n",
        "# technology/\n",
        "# chimeras/\n",
        "# mind-identity/\n",
        "# turing-test/\n",
        "# consciousness-animal/"
      ],
      "metadata": {
        "id": "hY5zTCRNMGNv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import BSHTMLLoader, DirectoryLoader\n",
        "subfolder = 'stanford_encyclopedia_of_philosophy'\n",
        "gdrive_data_docs_subdir = gdrive_data_docs + '/' + subfolder\n",
        "\n",
        "bshtml_dir_loader = DirectoryLoader(gdrive_data_docs_subdir, loader_cls=BSHTMLLoader)\n",
        "html_data = bshtml_dir_loader.load()"
      ],
      "metadata": {
        "id": "LKaWe1ZGGNnJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_html_data = \\\n",
        " [article for article in html_data if article.metadata['source'].split('/')[-1].replace('.html','').replace('_','-') in selected_sep_articles]\n",
        "len(selected_html_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka3oYJZUMexe",
        "outputId": "9152f67f-2104-4746-d266-b1792140b7e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### US White House Exec Order and EU AI Act pdf files"
      ],
      "metadata": {
        "id": "C6cdBtO1Qnk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    {\n",
        "        \"name\": \"us_regulations\",\n",
        "        \"path\": gdrive_data_docs + '/' + us_docs + '/' + \\\n",
        "           \"Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence _ United States, White House, November 2023.pdf\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"eu_regulations\",\n",
        "        \"path\": gdrive_data_docs + '/' + eu_docs + '/' + \\\n",
        "          \"EU_AI_act.pdf\",\n",
        "    }\n",
        "\n",
        "]\n"
      ],
      "metadata": {
        "id": "tQLT0wtTGxW0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_list = [item['path'] for item in files]\n",
        "src_dict = dict()\n",
        "src_dict = {item.split('/')[-1]:item for item in src_list }\n",
        "src_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pey_tpY3K8uF",
        "outputId": "ec43b75c-174b-45a8-b689-b04a93515cfb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence _ United States, White House, November 2023.pdf': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/us_regulations_etc/Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence _ United States, White House, November 2023.pdf',\n",
              " 'EU_AI_act.pdf': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/eu_regulations_etc/EU_AI_act.pdf'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pages = list()\n",
        "for file in files:\n",
        "    loader = PyPDFLoader(file[\"path\"])\n",
        "    pages += loader.load_and_split()"
      ],
      "metadata": {
        "id": "Zp23CJFHC7j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99SefYyOC-RV",
        "outputId": "1f523b20-7973-4be5-edb3-0da829f03a89"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "335"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! ls '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/us_regulations_etc'\n",
        "! ls '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/embeddings/'"
      ],
      "metadata": {
        "id": "sVPfMXDYLanJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI credentials etc"
      ],
      "metadata": {
        "id": "NQvvOjAsCdB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "OpenAI_token = getpass.getpass(prompt='Enter your OpenAI API token')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OpenAI_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL0bv4UjHasg",
        "outputId": "d7e70b67-af2b-48ec-c768-1fdfbf6c2c07"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API token··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Populate the vector store"
      ],
      "metadata": {
        "id": "o2Q4lrLkEDqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combine pdf and html documents and split into chunks"
      ],
      "metadata": {
        "id": "acicEWBqDGNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts = [*pages, *selected_html_data]\n",
        "len(all_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTvWaidfRHvp",
        "outputId": "e97bee5d-842d-4262-eec1-b487bafa1f13"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "345"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "chunked_documents = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "nDy3fOf4EKO5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunked_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhvutr38sbC0",
        "outputId": "7c0cecbb-57a9-4b7a-d330-f2e17bf6c451"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "335"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "rec_text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=20,separators = [\"\\n\\n\"])\n",
        "chunked_html = rec_text_splitter.split_documents(selected_html_data)\n",
        "len(chunked_html)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0Rc59DQnPH9",
        "outputId": "bea83ab6-2812-47de-86d9-689fa1f80b56"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "474"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_chunked_documents = [*chunked_documents, *chunked_html]\n",
        "len(all_chunked_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2taqPW9eVD6X",
        "outputId": "3189ff1a-fddd-4de0-98a5-ac2df3c46d50"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "809"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set up the persistant ChromaDB vector store"
      ],
      "metadata": {
        "id": "qfYhEvA8ajZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = gdrive_data_embs\n",
        "\n",
        "if len(os.listdir( persist_directory )) > 0:  # Check if the directory is empty\n",
        "    # Load existing ChromaDB collection\n",
        "    vectordb = Chroma.load(persist_directory)\n"
      ],
      "metadata": {
        "id": "DIZ9cZ2_Sh6b"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores import Chroma\n",
        "#from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "persist_directory = gdrive_data_embs\n",
        "\n",
        "if len(os.listdir( persist_directory )) > 0:  # Check if the directory is empty\n",
        "    # Load existing ChromaDB collection\n",
        "    vectordb = Chroma.load(persist_directory)\n",
        "else:\n",
        "    # Create a new ChromaDB collection\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=all_chunked_documents,\n",
        "        embedding=OpenAIEmbeddings(),\n",
        "        persist_directory = persist_directory\n",
        "    )\n",
        ""
      ],
      "metadata": {
        "id": "ALapyxW-IZVB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aEFZ_ByXMD8S"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up RAG pipeline"
      ],
      "metadata": {
        "id": "4J76e5ZHEafb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the future we might supply page numbers as references from the retrieval calls\n",
        "# page_numbers = [doc.metadata['page'] for doc in result['source_documents']]\n",
        "# page_numbers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou9MaHKKhiZi",
        "outputId": "1d5b152d-bb37-44a3-c748-f27a8b34a432"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[99, 18, 17, 106]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function retrieves chunks that are only from the specified source. Two tools are associated with the different sources"
      ],
      "metadata": {
        "id": "sEJIVqJRDtXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def source_aware_retriever(vectorstore, source):\n",
        "    def run(query):\n",
        "        doc_retriever = vectordb.as_retriever(search_kwargs={\"filter\": {\"source\": source}})\n",
        "\n",
        "        chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=doc_retriever, return_source_documents=False)\n",
        "        result = chain.invoke(query, metadata_filters={\"source\": source})\n",
        "        #result = chain.invoke(query) # We already filtered it upstream so we don't need to do it again.\n",
        "        return result\n",
        "    return run"
      ],
      "metadata": {
        "id": "wNXfZ6xCMSN4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = list(src_dict.keys())[0]\n",
        "source_a_tool = source_aware_retriever(vectordb, source=src_dict[src])\n",
        "\n",
        "src = list(src_dict.keys())[1]\n",
        "source_b_tool = source_aware_retriever(vectordb, source=src_dict[src])"
      ],
      "metadata": {
        "id": "RJakpeMmMcJq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A general retriever that doesn't specify the source"
      ],
      "metadata": {
        "id": "NRVSEuOVEF56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def general_retriever(vectorstore):\n",
        "    def run(query):\n",
        "        doc_retriever = vectordb.as_retriever(search_kwargs={\"k\":2})\n",
        "\n",
        "        chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=doc_retriever, return_source_documents=True)\n",
        "        result = chain.invoke(query)\n",
        "        return result\n",
        "    return run"
      ],
      "metadata": {
        "id": "0xAzWjopQn-y"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sources_tool = general_retriever(vectordb)"
      ],
      "metadata": {
        "id": "Y4kGQ4FikeW4"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine whether the question is asking for a binary comparison (between US and EU regulations).\n",
        "For now, it just looks for the keyword `compare` although this could be made more robust."
      ],
      "metadata": {
        "id": "Nj43a4tbEOZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_question(question):\n",
        "    # ... Implement your classification logic here...\n",
        "    is_comparative = False\n",
        "    if 'compare' in question.lower():\n",
        "      is_comparative = True\n",
        "    return is_comparative"
      ],
      "metadata": {
        "id": "uK9PdhQ7M_2q"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools derived from the functions defined above"
      ],
      "metadata": {
        "id": "nDahqttSQo9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "\n",
        "classifier_tool = Tool(\n",
        "        name = \"Classify question\",\n",
        "        func=classify_question,\n",
        "        description=\"useful for determining if a question is asking for a comparison.\"\n",
        "    )\n",
        "retriever_tool_source_a = Tool(\n",
        "        name = \"White House Executive Order, November 2023\",\n",
        "        func=source_a_tool,\n",
        "        description=\"useful for determining what the White House Executive Order has to say on the topic.\"\n",
        "    )\n",
        "retriever_tool_source_b = Tool(\n",
        "        name = \"European Union AI Act, March 2024\",\n",
        "        func=source_b_tool,\n",
        "        description=\"useful for determining what EU AI Act has to say on the topic.\"\n",
        "    )\n",
        "retriever_tool_all_sources = Tool(\n",
        "        name = \"White House Executive Order, EU AI Act, and Stanford Encyclopedia of Philosophy\",\n",
        "        func=all_sources_tool,\n",
        "        description=\"useful for determining what rigorous sources have to say on the topic.\"\n",
        ")"
      ],
      "metadata": {
        "id": "yR2nc1yCNFqa"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM setup, for text generation"
      ],
      "metadata": {
        "id": "sEmmiAeRQpvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "\n",
        "llm = OpenAI(temperature=0,  max_tokens=300)\n",
        "llm_longer_response = OpenAI( temperature=0,  max_tokens=350)"
      ],
      "metadata": {
        "id": "x86AEtuBNbJg"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt templates"
      ],
      "metadata": {
        "id": "w8D0vSVuQq-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparator_prompt_template = \\\n",
        "  PromptTemplate.from_template(\"\"\"You have the following responses on the issue of '{issue}':\n",
        "* {source_a_name}: {source_a_response}\n",
        "* {source_b_name}: {source_b_response}\n",
        "Analyze the differences in positions taken by the two sources.\n",
        "Clearly state where they agree, where they disagree, and any important nuances in their stances.\n",
        "(Aim for about 200 words, in completed sentences.)\n",
        "\"\"\")\n",
        "\n",
        "comparator_tool = LLMChain(llm=llm, prompt=comparator_prompt_template, verbose=True)"
      ],
      "metadata": {
        "id": "PyxY9NPRNrzq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_prompt_template = \\\n",
        "  PromptTemplate.from_template(\"\"\"You have the following responses on the issue of '{issue}':\n",
        "* {source_documents}\n",
        "Analyze the various angles on the issue, with respect to ethics, morality, and regulatory consisderations.\n",
        "Cite peoples' names (and publication dates) wherever possible.\n",
        "(Aim for about 300 words, in completed sentences.)\n",
        "\"\"\")\n",
        "\n",
        "#analysis_tool = LLMChain(llm=llm_longer_response, prompt=analysis_prompt_template, verbose=True)\n",
        "analysis_tool = LLMChain(llm=llm_longer_response, prompt=analysis_prompt_template, verbose=True)"
      ],
      "metadata": {
        "id": "u38zVqcTQrrc"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_completion_prompt_template = \\\n",
        "  PromptTemplate.from_template(\"\"\"Based on the preceding context '{context}', provide only the remainder of\n",
        "  the final incomplete sentence. Do not include any of the context already provided.\n",
        "  Return only the new part that the completes the sentence.\"\"\")\n",
        "sentence_completion_tool = LLMChain(llm=llm, prompt=sentence_completion_prompt_template, verbose=False)"
      ],
      "metadata": {
        "id": "kw6fyRlb5BpR"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The main event:  This function `run_agent` determines the flow of response generation\n",
        "\n",
        "• If the query is classified as a _comparison_ then it's going to retrieve chunks from the WH Exec Order and EU AI Act.\n",
        "Then a `comparator_tool` takes those chunks and synthesizes a response.\n",
        "\n",
        "• Otherwise all the sources, including a curated list of articles from the _Stanford Encyclopedia of Philosophy_, are queried.\n",
        "From the chunks returned, an response is synthesized.\n",
        "\n",
        "Either way, the summary returned often truncates in mid sentence, so a completion tool takes the last 7 complete sentences and generates a sentence completion for the final incomplete sentence and appends it to the original summary.\n",
        "\n"
      ],
      "metadata": {
        "id": "fvBFh8ys6ByM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_agent(query):\n",
        "    is_comparative = classifier_tool.run(query)\n",
        "    if is_comparative:\n",
        "        source_a_response = retriever_tool_source_a.invoke(query)\n",
        "        source_b_response = retriever_tool_source_b.invoke(query)\n",
        "\n",
        "        # Enhanced with {issue} extraction (if needed)\n",
        "        #issue = extract_issue_from_query(query)\n",
        "        issue = query\n",
        "\n",
        "        summary = comparator_tool.run(\n",
        "            source_a_name = retriever_tool_source_a.name,\n",
        "            source_a_response=source_a_response,\n",
        "\n",
        "            source_b_name = retriever_tool_source_b.name,\n",
        "            source_b_response=source_b_response,\n",
        "\n",
        "            issue=issue\n",
        "        )\n",
        "        #return summary\n",
        "    else:\n",
        "        # ... handle non-comparative queries ...\n",
        "        issue = query\n",
        "        all_sources_response = retriever_tool_all_sources(query)\n",
        "        print(len(all_sources_response))\n",
        "        summary = analysis_tool.run(\n",
        "          source_documents = all_sources_response,\n",
        "          issue=issue\n",
        "        )\n",
        "    if summary[-1] == '.':\n",
        "      return summary\n",
        "    else:\n",
        "      context = '. '.join( summary.split('.')[-7:])\n",
        "      completion = ' ' + sentence_completion_tool.run(context).strip('\\n\\n').strip('\\'')\n",
        "      return summary + \" \" + completion\n",
        "#"
      ],
      "metadata": {
        "id": "J__hBKARN3d1"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \"\"\"In addition to ethical considerations, there are also moral implications to be considered. The use of AI systems to manipulate behavior can have a significant impact on individuals and society as a whole. As mentioned by Woolley and Howard (2017), the use of social media for political propaganda can influence voting behavior and potentially harm the autonomy of individuals. This raises questions about the moral responsibility of companies and governments in using AI systems for manipulative purposes.\n",
        "From a regulatory perspective, there are also several considerations to be taken into account. As mentioned by Schneier (2015), the business model of the internet is based on surveillance and the collection of personal data. This raises concerns about the protection of privacy and the ownership of data. Wachter and\"\"\"\n",
        "\n",
        "context = '. '.join( summary.split('.')[-7:])\n",
        "completion = ' ' + sentence_completion_tool.run(context).strip('\\n\\n').strip('\\'')\n",
        "summary + \" \" + completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "pMzhQYctQskQ",
        "outputId": "8928b8d0-6259-4b3e-d2e7-9fc36417dd29"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In addition to ethical considerations, there are also moral implications to be considered. The use of AI systems to manipulate behavior can have a significant impact on individuals and society as a whole. As mentioned by Woolley and Howard (2017), the use of social media for political propaganda can influence voting behavior and potentially harm the autonomy of individuals. This raises questions about the moral responsibility of companies and governments in using AI systems for manipulative purposes.\\nFrom a regulatory perspective, there are also several considerations to be taken into account. As mentioned by Schneier (2015), the business model of the internet is based on surveillance and the collection of personal data. This raises concerns about the protection of privacy and the ownership of data. Wachter and  Mittelstadt (2018) argue that there is a need for transparency and accountability in the use of AI systems, as well as clear guidelines for ethical and responsible use.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstrate RAG pipeline"
      ],
      "metadata": {
        "id": "iF4b-TpdEDZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "0fjRI0X3BHxC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"We might add a feature to our AI system that some say manipulates behavior. What should we consider?\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "ckN2HM6I0DGQ",
        "outputId": "9b29f548-df3e-448d-a37f-39aba1730de6"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'We might add a feature to our AI system that some say manipulates behavior. What should we consider?':\n",
            "* {'query': 'We might add a feature to our AI system that some say manipulates behavior. What should we consider?', 'result': ' The ethical implications and potential consequences of manipulating behavior through AI should be carefully considered.', 'source_documents': [Document(page_content='The ethical issues of AI in surveillance go beyond the mere\\naccumulation of data and direction of attention: They include\\nthe use of information to manipulate behaviour, online and\\noffline, in a way that undermines autonomous rational choice. Of\\ncourse, efforts to manipulate behaviour are ancient, but they may gain\\na new quality when they use AI systems. Given users’ intense\\ninteraction with data systems and the deep knowledge about individuals\\nthis provides, they are vulnerable to “nudges”,\\nmanipulation, and deception. With sufficient prior data, algorithms\\ncan be used to target individuals or small groups with just the kind\\nof input that is likely to influence these particular individuals. A\\n’nudge‘ changes the environment such that it influences\\nbehaviour in a predictable way that is positive for the individual,\\nbut easy and cheap to avoid (Thaler & Sunstein 2008). There is a\\nslippery slope from here to paternalism and manipulation.\\n\\nMany advertisers, marketers, and online sellers will use any legal\\nmeans at their disposal to maximise profit, including exploitation of\\nbehavioural biases, deception, and addiction generation (Costa and\\nHalpern 2019 [OIR]). Such manipulation is the business model in much\\nof the gambling and gaming industries, but it is spreading, e.g., to\\nlow-cost airlines. In interface design on web pages or in games, this\\nmanipulation uses what is called “dark patterns” (Mathur\\net al. 2019). At this moment, gambling and the sale of addictive\\nsubstances are highly regulated, but online manipulation and addiction\\nare not—even though manipulation of online behaviour is becoming\\na core business model of the Internet.\\n\\nFurthermore, social media is now the prime location for political\\npropaganda. This influence can be used to steer voting behaviour, as\\nin the Facebook-Cambridge Analytica “scandal” (Woolley and\\nHoward 2017; Bradshaw, Neudert, and Howard 2019) and—if\\nsuccessful—it may harm the autonomy of individuals (Susser,\\nRoessler, and Nissenbaum 2019).\\n\\nImproved AI “faking” technologies make what once was\\nreliable evidence into unreliable evidence—this has already\\nhappened to digital photos, sound recordings, and video. It will soon\\nbe quite easy to create (rather than alter) “deep fake”\\ntext, photos, and video material with any desired content. Soon,\\nsophisticated real-time interaction with persons over text, phone, or\\nvideo will be faked, too. So we cannot trust digital interactions\\nwhile we are at the same time increasingly dependent on such\\ninteractions.\\n\\nOne more specific issue is that machine learning techniques in AI rely\\non training with vast amounts of data. This means there will often be\\na trade-off between privacy and rights to data vs. technical quality\\nof the product. This influences the consequentialist evaluation of\\nprivacy-violating practices.', metadata={'source': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/stanford_encyclopedia_of_philosophy/ethics-ai.html', 'title': '\\nEthics of Artificial Intelligence and Robotics (Stanford Encyclopedia of Philosophy)\\n'}), Document(page_content='At the same time, controlling who collects which data, and who has\\naccess, is much harder in the digital world than it was in the\\nanalogue world of paper and telephone calls. Many new AI technologies\\namplify the known issues. For example, face recognition in photos and\\nvideos allows identification and thus profiling and searching for\\nindividuals (Whittaker et al. 2018: 15ff). This continues using other\\ntechniques for identification, e.g., “device\\nfingerprinting”, which are commonplace on the Internet\\n(sometimes revealed in the “privacy policy”). The result\\nis that “In this vast ocean of data, there is a frighteningly\\ncomplete picture of us” (Smolan 2016: 1:01). The result is\\narguably a scandal that still has not received due public\\nattention.\\n\\nThe data trail we leave behind is how our “free” services\\nare paid for—but we are not told about that data collection and\\nthe value of this new raw material, and we are manipulated into\\nleaving ever more such data. For the “big 5” companies\\n(Amazon, Google/Alphabet, Microsoft, Apple, Facebook), the main\\ndata-collection part of their business appears to be based on\\ndeception, exploiting human weaknesses, furthering procrastination,\\ngenerating addiction, and manipulation (Harris 2016 [OIR]). The\\nprimary focus of social media, gaming, and most of the Internet in\\nthis “surveillance economy” is to gain, maintain, and\\ndirect attention—and thus data supply. “Surveillance is\\nthe business model of the Internet” (Schneier 2015). This\\nsurveillance and attention economy is sometimes called\\n“surveillance capitalism” (Zuboff 2019). It has caused\\nmany attempts to escape from the grasp of these corporations, e.g., in\\nexercises of “minimalism” (Newport 2019), sometimes\\nthrough the open source movement, but it appears that present-day\\ncitizens have lost the degree of autonomy needed to escape while fully\\ncontinuing with their life and work. We have lost ownership of our\\ndata, if “ownership” is the right relation here. Arguably,\\nwe have lost control of our data.\\n\\nThese systems will often reveal facts about us that we ourselves wish\\nto suppress or are not aware of: they know more about us than we know\\nourselves. Even just observing online behaviour allows insights into\\nour mental states (Burr and Christianini 2019) and manipulation (see\\nbelow\\n section 2.2).\\n This has led to calls for the protection of “derived\\ndata” (Wachter and Mittelstadt 2019). With the last sentence of\\nhis bestselling book, Homo Deus, Harari asks about the\\nlong-term consequences of AI:\\n\\n\\nWhat will happen to society, politics and daily life when\\nnon-conscious but highly intelligent algorithms know us better than we\\nknow ourselves? (2016: 462)', metadata={'source': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/stanford_encyclopedia_of_philosophy/ethics-ai.html', 'title': '\\nEthics of Artificial Intelligence and Robotics (Stanford Encyclopedia of Philosophy)\\n'})]} \n",
            "Analyze the various angles on the issue, with respect to ethics, morality, and regulatory consisderations. \n",
            "Cite peoples' names (and publication dates) wherever possible. \n",
            "(Aim for about 300 words, in completed sentences.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nThe issue of adding a feature to an AI system that manipulates behavior raises several ethical, moral, and regulatory considerations. One of the main ethical concerns is the potential consequences of manipulating behavior through AI. As mentioned in the source documents, this can undermine autonomous rational choice and harm the autonomy of individuals (Susser, Roessler, and Nissenbaum, 2019). This raises questions about the ethical responsibility of those developing and implementing such a feature, as well as the potential harm it could cause to individuals and society as a whole.\n\nFrom a moral perspective, the use of AI to manipulate behavior raises questions about the intentions and motivations behind such actions. As mentioned in the source documents, many advertisers and marketers already use manipulation and deception to maximize profit (Costa and Halpern, 2019). Adding AI to this equation could potentially amplify these unethical practices and further exploit human weaknesses. This raises moral concerns about the impact on individuals and society, as well as the responsibility of those involved in the development and use of such technology.\n\nIn terms of regulatory considerations, the use of AI to manipulate behavior raises questions about privacy and data protection. As mentioned in the source documents, AI systems rely on vast amounts of data for training, which can often involve a trade-off between privacy and technical quality (Wachter and Mittelstadt, 2019). This highlights the need for regulations that protect individuals' privacy and rights to their data, while also ensuring the quality and safety of AI systems.\n\nFurthermore, the use of AI to manipulate behavior also raises concerns about the potential for discrimination and bias. As AI systems are trained on existing data, they can perpetuate and amplify existing biases and discrimination (Burr and Christianini, 2019  , which can have harmful consequences for marginalized groups and perpetuate systemic inequalities. This highlights the need for regulations that address and mitigate bias in AI systems, as well as promote diversity and inclusivity in the development and use of such technology."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"Are there different AI bias considerations to worry about in the US and Europe?\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "G3IMVv6c8t88",
        "outputId": "1ff6497b-4f04-4221-8614-62d45facf98e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'Are there different AI bias considerations to worry about in the US and Europe?':\n",
            "* {'query': 'Are there different AI bias considerations to worry about in the US and Europe?', 'result': ' Yes, there are different AI bias considerations in the US and Europe. The EU has stricter privacy regulations, while the US and China prioritize growth with less regulation. The political dimensions of automated systems in the US are also being investigated.', 'source_documents': [Document(page_content='The policy in this field has its ups and downs: Civil liberties and\\nthe protection of individual rights are under intense pressure from\\nbusinesses’ lobbying, secret services, and other state agencies\\nthat depend on surveillance. Privacy protection has diminished\\nmassively compared to the pre-digital age when communication was based\\non letters, analogue telephone communications, and personal\\nconversation and when surveillance operated under significant legal\\nconstraints.\\n\\nWhile the EU General Data Protection Regulation (Regulation (EU)\\n2016/679) has strengthened privacy protection, the US and China prefer\\ngrowth with less regulation (Thompson and Bremmer 2018), likely in the\\nhope that this provides a competitive advantage. It is clear that\\nstate and business actors have increased their ability to invade\\nprivacy and manipulate people with the help of AI technology and will\\ncontinue to do so to further their particular interests—unless\\nreined in by policy in the interest of general society.\\n2.3 Opacity of AI Systems\\n\\nOpacity and bias are central issues in what is now sometimes called\\n“data ethics” or “big data ethics” (Floridi\\nand Taddeo 2016; Mittelstadt and Floridi 2016). AI systems for\\nautomated decision support and “predictive analytics”\\nraise “significant concerns about lack of due process,\\naccountability, community engagement, and auditing” (Whittaker\\net al. 2018: 18ff). They are part of a power structure in which\\n“we are creating decision-making processes that constrain and\\nlimit opportunities for human participation” (Danaher 2016b:\\n245). At the same time, it will often be impossible for the affected\\nperson to know how the system came to this output, i.e., the system is\\n“opaque” to that person. If the system involves machine\\nlearning, it will typically be opaque even to the expert, who will not\\nknow how a particular pattern was identified, or even what the pattern\\nis. Bias in decision systems and data sets is exacerbated by this\\nopacity. So, at least in cases where there is a desire to remove bias,\\nthe analysis of opacity and bias go hand in hand, and political\\nresponse has to tackle both issues together.', metadata={'source': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/stanford_encyclopedia_of_philosophy/ethics-ai.html', 'title': '\\nEthics of Artificial Intelligence and Robotics (Stanford Encyclopedia of Philosophy)\\n'}), Document(page_content='Bias typically surfaces when unfair judgments are made because the\\nindividual making the judgment is influenced by a characteristic that\\nis actually irrelevant to the matter at hand, typically a\\ndiscriminatory preconception about members of a group. So, one form of\\nbias is a learned cognitive feature of a person, often not made\\nexplicit. The person concerned may not be aware of having that\\nbias—they may even be honestly and explicitly opposed to a bias\\nthey are found to have (e.g., through priming, cf. Graham and Lowery\\n2004). On fairness vs. bias in machine learning, see Binns (2018).\\n\\nApart from the social phenomenon of learned bias, the human cognitive\\nsystem is generally prone to have various kinds of “cognitive\\nbiases”, e.g., the “confirmation bias”: humans tend\\nto interpret information as confirming what they already believe. This\\nsecond form of bias is often said to impede performance in rational\\njudgment (Kahnemann 2011)—though at least some cognitive biases\\ngenerate an evolutionary advantage, e.g., economical use of resources\\nfor intuitive judgment. There is a question whether AI systems could\\nor should have such cognitive bias.\\n\\nA third form of bias is present in data when it exhibits systematic\\nerror, e.g., “statistical bias”. Strictly, any given\\ndataset will only be unbiased for a single kind of issue, so the mere\\ncreation of a dataset involves the danger that it may be used for a\\ndifferent kind of issue, and then turn out to be biased for that kind.\\nMachine learning on the basis of such data would then not only fail to\\nrecognise the bias, but codify and automate the “historical\\nbias”. Such historical bias was discovered in an automated\\nrecruitment screening system at Amazon (discontinued early 2017) that\\ndiscriminated against women—presumably because the company had a\\nhistory of discriminating against women in the hiring process. The\\n“Correctional Offender Management Profiling for Alternative\\nSanctions” (COMPAS), a system to predict whether a defendant\\nwould re-offend, was found to be as successful (65.2% accuracy) as a\\ngroup of random humans (Dressel and Farid 2018) and to produce more\\nfalse positives and less false negatives for black defendants. The\\nproblem with such systems is thus bias plus humans placing excessive\\ntrust in the systems. The political dimensions of such automated\\nsystems in the USA are investigated in Eubanks (2018).\\n\\nThere are significant technical efforts to detect and remove bias from\\nAI systems, but it is fair to say that these are in early stages: see\\nUK Institute for Ethical AI & Machine Learning (Brownsword,\\nScotford, and Yeung 2017; Yeung and Lodge 2019). It appears that\\ntechnological fixes have their limits in that they need a mathematical\\nnotion of fairness, which is hard to come by (Whittaker et al. 2018:\\n24ff; Selbst et al. 2019), as is a formal notion of “race”\\n(see Benthall and Haynes 2019). An institutional proposal is in (Veale\\nand Binns 2017).\\n2.5 Human-Robot Interaction', metadata={'source': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/stanford_encyclopedia_of_philosophy/ethics-ai.html', 'title': '\\nEthics of Artificial Intelligence and Robotics (Stanford Encyclopedia of Philosophy)\\n'})]} \n",
            "Analyze the various angles on the issue, with respect to ethics, morality, and regulatory consisderations. \n",
            "Cite peoples' names (and publication dates) wherever possible. \n",
            "(Aim for about 300 words, in completed sentences.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nThe issue of AI bias considerations in the US and Europe raises various ethical, moral, and regulatory concerns. One of the main ethical concerns is the potential for discrimination and unfair treatment of individuals based on their race, gender, or other characteristics. This is a concern in both the US and Europe, as evidenced by the examples of biased AI systems in both regions. In the US, the COMPAS system was found to be biased against black defendants, while in Europe, the EU General Data Protection Regulation was implemented to address privacy concerns and potential discrimination.\n\nFrom a moral perspective, the use of AI systems that perpetuate bias and discrimination raises questions about the values and principles that underpin these systems. As mentioned in the source documents, the human cognitive system is prone to various biases, and there is a question of whether AI systems should also have these biases. This raises moral questions about the responsibility of developers and policymakers to ensure that AI systems are designed and used in an ethical and fair manner.\n\nIn terms of regulatory considerations, there are differences between the US and Europe. The EU has stricter privacy regulations, while the US and China prioritize growth with less regulation. This highlights the different approaches to AI regulation in these regions. In the US, there is a focus on promoting innovation and economic growth, while in Europe, there is a greater emphasis on protecting individual rights and privacy. This raises questions about the role of government in regulating AI and the balance between promoting innovation and protecting individuals from potential harm.\n\nFurthermore, the political dimensions of automated systems in the US are also being investigated, as mentioned in the source documents. This highlights the need for regulatory frameworks that not only address technical aspects of AI but also consider the societal and political implications of these systems.\n\nIn conclusion  the need for comprehensive and thoughtful regulation of AI that takes into account both technical and societal factors."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"We might add a feature to our AI system that some say manipulates behavior. What should we consider?\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "eK-40d_20ZkG",
        "outputId": "be2d447f-303c-411e-ea1f-55000446ca1f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'We might add a feature to our AI system that some say manipulates behavior. What should we consider?':\n",
            "* {'query': 'We might add a feature to our AI system that some say manipulates behavior. What should we consider?', 'result': ' The ethical implications and potential consequences of manipulating behavior through AI should be carefully considered.', 'source_documents': [Document(page_content='The ethical issues of AI in surveillance go beyond the mere\\naccumulation of data and direction of attention: They include\\nthe use of information to manipulate behaviour, online and\\noffline, in a way that undermines autonomous rational choice. Of\\ncourse, efforts to manipulate behaviour are ancient, but they may gain\\na new quality when they use AI systems. Given users’ intense\\ninteraction with data systems and the deep knowledge about individuals\\nthis provides, they are vulnerable to “nudges”,\\nmanipulation, and deception. With sufficient prior data, algorithms\\ncan be used to target individuals or small groups with just the kind\\nof input that is likely to influence these particular individuals. A\\n’nudge‘ changes the environment such that it influences\\nbehaviour in a predictable way that is positive for the individual,\\nbut easy and cheap to avoid (Thaler & Sunstein 2008). There is a\\nslippery slope from here to paternalism and manipulation.\\n\\nMany advertisers, marketers, and online sellers will use any legal\\nmeans at their disposal to maximise profit, including exploitation of\\nbehavioural biases, deception, and addiction generation (Costa and\\nHalpern 2019 [OIR]). Such manipulation is the business model in much\\nof the gambling and gaming industries, but it is spreading, e.g., to\\nlow-cost airlines. In interface design on web pages or in games, this\\nmanipulation uses what is called “dark patterns” (Mathur\\net al. 2019). At this moment, gambling and the sale of addictive\\nsubstances are highly regulated, but online manipulation and addiction\\nare not—even though manipulation of online behaviour is becoming\\na core business model of the Internet.\\n\\nFurthermore, social media is now the prime location for political\\npropaganda. This influence can be used to steer voting behaviour, as\\nin the Facebook-Cambridge Analytica “scandal” (Woolley and\\nHoward 2017; Bradshaw, Neudert, and Howard 2019) and—if\\nsuccessful—it may harm the autonomy of individuals (Susser,\\nRoessler, and Nissenbaum 2019).\\n\\nImproved AI “faking” technologies make what once was\\nreliable evidence into unreliable evidence—this has already\\nhappened to digital photos, sound recordings, and video. It will soon\\nbe quite easy to create (rather than alter) “deep fake”\\ntext, photos, and video material with any desired content. Soon,\\nsophisticated real-time interaction with persons over text, phone, or\\nvideo will be faked, too. So we cannot trust digital interactions\\nwhile we are at the same time increasingly dependent on such\\ninteractions.\\n\\nOne more specific issue is that machine learning techniques in AI rely\\non training with vast amounts of data. This means there will often be\\na trade-off between privacy and rights to data vs. technical quality\\nof the product. This influences the consequentialist evaluation of\\nprivacy-violating practices.', metadata={'source': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/stanford_encyclopedia_of_philosophy/ethics-ai.html', 'title': '\\nEthics of Artificial Intelligence and Robotics (Stanford Encyclopedia of Philosophy)\\n'}), Document(page_content='At the same time, controlling who collects which data, and who has\\naccess, is much harder in the digital world than it was in the\\nanalogue world of paper and telephone calls. Many new AI technologies\\namplify the known issues. For example, face recognition in photos and\\nvideos allows identification and thus profiling and searching for\\nindividuals (Whittaker et al. 2018: 15ff). This continues using other\\ntechniques for identification, e.g., “device\\nfingerprinting”, which are commonplace on the Internet\\n(sometimes revealed in the “privacy policy”). The result\\nis that “In this vast ocean of data, there is a frighteningly\\ncomplete picture of us” (Smolan 2016: 1:01). The result is\\narguably a scandal that still has not received due public\\nattention.\\n\\nThe data trail we leave behind is how our “free” services\\nare paid for—but we are not told about that data collection and\\nthe value of this new raw material, and we are manipulated into\\nleaving ever more such data. For the “big 5” companies\\n(Amazon, Google/Alphabet, Microsoft, Apple, Facebook), the main\\ndata-collection part of their business appears to be based on\\ndeception, exploiting human weaknesses, furthering procrastination,\\ngenerating addiction, and manipulation (Harris 2016 [OIR]). The\\nprimary focus of social media, gaming, and most of the Internet in\\nthis “surveillance economy” is to gain, maintain, and\\ndirect attention—and thus data supply. “Surveillance is\\nthe business model of the Internet” (Schneier 2015). This\\nsurveillance and attention economy is sometimes called\\n“surveillance capitalism” (Zuboff 2019). It has caused\\nmany attempts to escape from the grasp of these corporations, e.g., in\\nexercises of “minimalism” (Newport 2019), sometimes\\nthrough the open source movement, but it appears that present-day\\ncitizens have lost the degree of autonomy needed to escape while fully\\ncontinuing with their life and work. We have lost ownership of our\\ndata, if “ownership” is the right relation here. Arguably,\\nwe have lost control of our data.\\n\\nThese systems will often reveal facts about us that we ourselves wish\\nto suppress or are not aware of: they know more about us than we know\\nourselves. Even just observing online behaviour allows insights into\\nour mental states (Burr and Christianini 2019) and manipulation (see\\nbelow\\n section 2.2).\\n This has led to calls for the protection of “derived\\ndata” (Wachter and Mittelstadt 2019). With the last sentence of\\nhis bestselling book, Homo Deus, Harari asks about the\\nlong-term consequences of AI:\\n\\n\\nWhat will happen to society, politics and daily life when\\nnon-conscious but highly intelligent algorithms know us better than we\\nknow ourselves? (2016: 462)', metadata={'source': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/stanford_encyclopedia_of_philosophy/ethics-ai.html', 'title': '\\nEthics of Artificial Intelligence and Robotics (Stanford Encyclopedia of Philosophy)\\n'})]} \n",
            "Analyze the various angles on the issue, with respect to ethics, morality, and regulatory consisderations. \n",
            "Cite peoples' names (and publication dates) wherever possible. \n",
            "(Aim for about 300 words, in completed sentences.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nThe issue of adding a feature to an AI system that manipulates behavior raises several ethical, moral, and regulatory considerations. One of the main ethical concerns is the potential consequences of manipulating behavior through AI. As mentioned in the source documents, this can undermine autonomous rational choice and harm the autonomy of individuals (Susser, Roessler, and Nissenbaum, 2019). This raises questions about the ethical responsibility of those developing and implementing such a feature, as well as the potential harm it could cause to individuals and society as a whole.\n\nFrom a moral perspective, the use of AI to manipulate behavior raises questions about the intentions and motivations behind such actions. As mentioned in the source documents, many advertisers and marketers already use manipulation and deception to maximize profit (Costa and Halpern, 2019). Adding AI to this equation could potentially amplify these unethical practices and further exploit human weaknesses. This raises moral concerns about the impact on individuals and society, as well as the responsibility of those involved in the development and use of such technology.\n\nIn terms of regulatory considerations, the use of AI to manipulate behavior raises questions about privacy and data protection. As mentioned in the source documents, AI systems rely on vast amounts of data for training, which can often involve a trade-off between privacy and technical quality (Wachter and Mittelstadt, 2019). This highlights the need for regulations that protect individuals' privacy and rights to their data, while also ensuring the quality and safety of AI systems.\n\nFurthermore, the use of AI to manipulate behavior also raises concerns about the potential for discrimination and bias. As AI systems are trained on existing data, they can perpetuate and amplify existing biases and discrimination (Burr and Christianini, 2019"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"What are the issues the might be considered if we add 'device fingerprinting' to our AI system?\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "KQ_38SzKqAs4",
        "outputId": "6804a713-77c6-4e8d-e749-170a38a6286b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nThe addition of 'device fingerprinting' to AI systems raises ethical concerns about privacy, profiling, manipulation, and loss of control over personal data. These issues have already been amplified by other AI technologies, such as face recognition and surveillance, and may continue to be a source of controversy and public attention. Additionally, the use of AI in device fingerprinting can further exacerbate issues of opacity and bias in decision-making processes. Therefore, it is crucial for policy makers to carefully consider and regulate the use of device fingerprinting in AI systems, as well as ensure responsible design to protect privacy and address potential biases.\n\nOne of the main ethical concerns surrounding the use of device fingerprinting in AI systems is the violation of privacy. As mentioned in the source documents, the digital world has made it much harder to control who collects and has access to personal data. This raises questions about the ownership and control of personal data, as well as the potential for manipulation and exploitation by businesses and state agencies. This issue has been further complicated by the use of AI in device fingerprinting, which can collect and analyze vast amounts of data without the knowledge or consent of individuals.\n\nAnother ethical consideration is the potential for bias in decision-making processes. AI systems rely on vast amounts of data to make decisions, and this data may contain inherent biases. This can lead to discriminatory outcomes, particularly in areas such as employment and criminal justice. The use of device fingerprinting in AI systems can exacerbate this issue, as it may rely on data that is already biased due to factors such as socioeconomic status or race.\n\nFrom a moral standpoint, the use of device fingerprinting in AI systems raises questions about the autonomy and agency of individuals. As AI systems become more advanced and capable of making decisions on behalf of humans, there is a concern that individuals may lose control over their own lives. This is particularly relevant in the context of device fingerprinting, as it can track and analyze personal data without the knowledge or consent of individuals.\n\nIn terms of regulatory considerations, there is a need for policies and laws to address the ethical concerns surrounding device fingerprinting in AI systems. This includes regulations on data collection and usage, as well as guidelines for responsible design and development of AI systems. Additionally, there is a need for transparency and accountability in the use of AI, particularly in regards to decision-making processes that may have significant impacts on individuals and society as a whole.\n\nIn conclusion, the addition of device fingerprinting to AI systems raises a range of ethical,"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"What should we consider as, or if, we try to program morality into autonomous weapons systems or vehicles?\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "1hndKk_2sLUM",
        "outputId": "a1a5b622-c2a4-4593-e3c2-22048ef6e160"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'What should we consider as, or if, we try to program morality into autonomous weapons systems or vehicles?':\n",
            "* {'query': 'What should we consider as, or if, we try to program morality into autonomous weapons systems or vehicles?', 'result': ' When considering programming morality into autonomous weapons systems or vehicles, we should consider questions on how they should behave and how responsibility and risk should be distributed in the complicated system they operate in.', 'source_documents': [Document(page_content='In the absence of any definitive arguments for or against the\\npossibility of future computer systems being morally responsible,\\nresearchers within the field of machine ethics aim to further develop\\nthe discussion by focusing instead on creating computer system that\\ncan behave as if they are moral agents (Moor 2006,\\nCervantes et al 2019 , Zoshak and Dew 2021). Research\\nwithin this field has been concerned with the design and development\\nof computer systems that can independently determine what the right\\nthing to do would be in a given situation. According to Allen and\\nWallach, such autonomous moral agents (AMAs) would have to be\\ncapable of reasoning about the moral and social significance of their\\nbehavior and use their assessment of the effects their behavior has on\\nsentient beings to make appropriate choices (2012; see also Wallach\\nand Allen 2009 and Allen et al. 2000). Such abilities are needed, they\\nargue, because computers are becoming more and more complex and\\ncapable of operating without direct human control in different\\ncontexts and environments. Progressively autonomous technologies\\nalready in development, such as military robots, driverless cars or\\ntrains and service robots in the home and for healthcare, will be\\ninvolved in moral situations that directly affect the safety and\\nwell-being of humans. An autonomous bomb disposal robot might in the\\nfuture be faced with the decision which bomb it should defuse first,\\nin order to minimize casualties. Similarly, a moral decision that a\\ndriverless car might have to make is whether to break for a crossing\\ndog or avoid the risk of causing injury to the driver behind him. Such\\ndecisions require judgment. Currently operators make such moral\\ndecisions, or the decision is already inscribed in the design of the\\ncomputer system. Machine ethics, Wallach and Allen argue, goes one\\nstep beyond making engineers aware of the values they build into the\\ndesign of their products, as it seeks to build ethical decision-making\\ninto the machines.', metadata={'source': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/stanford_encyclopedia_of_philosophy/computing-responsibility.html', 'title': '\\nComputing and Moral Responsibility (Stanford Encyclopedia of Philosophy)\\n'}), Document(page_content='Generally speaking, one question is the degree to which autonomous\\nrobots raise issues our present conceptual schemes must adapt to, or\\nwhether they just require technical adjustments. In most\\njurisdictions, there is a sophisticated system of civil and criminal\\nliability to resolve such issues. Technical standards, e.g., for the\\nsafe use of machinery in medical environments, will likely need to be\\nadjusted. There is already a field of “verifiable AI” for\\nsuch safety-critical systems and for “security\\napplications”. Bodies like the IEEE (The Institute of Electrical\\nand Electronics Engineers) and the BSI (British Standards Institution)\\nhave produced “standards”, particularly on more technical\\nsub-problems, such as data security and transparency. Among the many\\nautonomous systems on land, on water, under water, in air or space, we\\ndiscuss two samples: autonomous vehicles and autonomous weapons.\\n2.7.1 Example (a) Autonomous Vehicles\\n\\nAutonomous vehicles hold the promise to reduce the very significant\\ndamage that human driving currently causes—approximately 1\\nmillion humans being killed per year, many more injured, the\\nenvironment polluted, earth sealed with concrete and tarmac, cities\\nfull of parked cars, etc. However, there seem to be questions on how\\nautonomous vehicles should behave, and how responsibility and risk\\nshould be distributed in the complicated system the vehicles operates\\nin. (There is also significant disagreement over how long the\\ndevelopment of fully autonomous, or “level 5” cars (SAE\\nInternational 2018) will actually take.)', metadata={'source': '/content/gdrive/My Drive/ai_regulations_and_ethics_navigation/documents/stanford_encyclopedia_of_philosophy/ethics-ai.html', 'title': '\\nEthics of Artificial Intelligence and Robotics (Stanford Encyclopedia of Philosophy)\\n'})]} \n",
            "Analyze the various angles on the issue, with respect to ethics, morality, and regulatory consisderations. \n",
            "Cite peoples' names (and publication dates) wherever possible. \n",
            "(Aim for about 320 words, in completed sentences.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nThe issue of programming morality into autonomous weapons systems or vehicles raises a number of ethical, moral, and regulatory considerations. One key question is how these systems should behave and make decisions in morally complex situations. This has been a topic of discussion in the field of machine ethics, with researchers aiming to create computer systems that can behave as if they are moral agents (Moor, 2006; Cervantes et al., 2019; Zoshak & Dew, 2021). This involves designing systems that can independently determine the right course of action in a given situation, taking into account the moral and social implications of their behavior.\n\nAnother important consideration is the distribution of responsibility and risk in these systems. As autonomous technologies become more complex and capable of operating without direct human control, there is a need to determine who is responsible for the actions of these systems. Allen and Wallach (2012) argue that autonomous moral agents (AMAs) must be able to reason about the moral and social significance of their behavior and use this information to make appropriate choices. This is particularly relevant in contexts such as military robots, driverless cars, and service robots, where the decisions made by these systems can directly impact the safety and well-being of humans.\n\nIn terms of regulatory considerations, there is a need to adapt existing laws and standards to accommodate autonomous systems. This includes civil and criminal liability, as well as technical standards for safety and security. Bodies like the IEEE and BSI have already produced standards"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n2_tPM909qGu"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare US and EU regulations\n",
        "#### Use the keyword `compare` to signal a question concering comparing the White House Executive order versus the EU AI Act"
      ],
      "metadata": {
        "id": "H0nUY8JG90r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"Compare the US and European stances on education with regard to AI technology\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "-jL7etNl_OVM",
        "outputId": "4398318f-c35c-4eff-f0db-8b301172a171"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'Compare the US and European stances on education with regard to AI technology':\n",
            "* White House Executive Order, November 2023: {'query': 'Compare the US and European stances on education with regard to AI technology', 'result': \" The US and European stances on education with regard to AI technology differ in some ways. The US has a more proactive approach, with a focus on promoting AI education and training in schools and universities, as well as investing in research and development. The European approach is more cautious, with a focus on regulating and managing the risks of AI, including potential job displacement and discrimination. Both regions recognize the importance of educating the public about AI and its potential impact, but the US tends to prioritize promoting innovation and competitiveness, while Europe prioritizes protecting citizens' rights and privacy.\"} \n",
            "* European Union AI Act, March 2024: {'query': 'Compare the US and European stances on education with regard to AI technology', 'result': ' The US and European stances on education with regard to AI technology differ in some ways. In the US, there is a strong emphasis on promoting and supporting research and development of AI solutions for socially and environmentally beneficial outcomes. This includes allocating resources and funding for projects that aim to increase accessibility for persons with disabilities, tackle socio-economic inequalities, and meet environmental targets. In contrast, the European stance also emphasizes the importance of promoting and protecting innovation, but also places a strong emphasis on safeguarding against potential harms and risks associated with AI technology. This includes classifying certain AI systems used in education, such as those used for evaluating learning outcomes or monitoring student behavior, as high-risk and subject to additional safeguards and oversight. Additionally, the European stance highlights the potential for AI systems to perpetuate historical patterns of discrimination and calls for interdisciplinary cooperation between AI developers and experts on inequality and non-discrimination. Overall, while both the US and Europe prioritize the use of AI technology for beneficial outcomes, the European stance also emphasizes the need for responsible and ethical development and use of AI in education.'}\n",
            "Analyze the differences in positions taken by the two sources. \n",
            "Clearly state where they agree, where they disagree, and any important nuances in their stances. \n",
            "(Aim for about 200 words, in completed sentences.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nBoth the White House Executive Order and the European Union AI Act recognize the importance of education in the development and use of AI technology. They both agree that educating the public about AI and its potential impact is crucial. However, they differ in their approaches and priorities.\n\nThe White House Executive Order highlights the US's proactive approach towards promoting AI education and training in schools and universities. It also emphasizes the importance of investing in research and development to maintain the US's competitiveness in the global AI market. In contrast, the European Union AI Act takes a more cautious approach, with a focus on regulating and managing the potential risks and harms of AI technology. This includes classifying certain AI systems used in education as high-risk and subjecting them to additional safeguards and oversight.\n\nOne key difference between the two sources is their emphasis on protecting citizens' rights and privacy. While the US prioritizes promoting innovation and competitiveness, the European Union places a strong emphasis on safeguarding against potential harms and discrimination. This is evident in the European Union AI Act's call for interdisciplinary cooperation between AI developers and experts on inequality and non-discrimination.\n\nAnother important nuance in their stances is their focus on socially and environmentally beneficial outcomes. The US Executive Order highlights the importance of using AI technology to address issues such as accessibility for persons with disabilities, socio-economic inequalities, and environmental targets. On the other hand, the European Union AI Act also emphasizes the need for responsible and ethical development and use of AI in education, but does not explicitly mention  the potential positive impacts on society and the environment."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"Compare the US and European stances on education with regard to AI technology\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "X8Qnc48B-r6s",
        "outputId": "1f8aba88-89a2-4e19-a247-2dbed0f13258"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'Compare the US and European stances on education with regard to AI technology':\n",
            "* White House Executive Order, November 2023: {'query': 'Compare the US and European stances on education with regard to AI technology', 'result': \" The US and European stances on education with regard to AI technology differ in some ways. The US has a more proactive approach, with a focus on promoting AI education and training in schools and universities, as well as investing in research and development. The European approach is more cautious, with a focus on regulating and managing the risks of AI, including potential job displacement and discrimination. Both regions recognize the importance of educating the public about AI and its potential impact, but the US tends to prioritize promoting innovation and competitiveness, while Europe prioritizes protecting citizens' rights and privacy.\"} \n",
            "* European Union AI Act, March 2024: {'query': 'Compare the US and European stances on education with regard to AI technology', 'result': \" The US and European stances on education with regard to AI technology differ in some ways. In the US, there is a strong emphasis on promoting and supporting research and development of AI solutions for socially and environmentally beneficial outcomes. This includes allocating resources and funding for projects that aim to increase accessibility for persons with disabilities, tackle socio-economic inequalities, and meet environmental targets. However, there is less focus on regulating the use of AI systems in education, with the exception of high-risk systems that may determine a person's educational and professional course.\\n\\nIn contrast, the European stance on education and AI technology is more focused on regulation and protection of individuals' rights. The EU is working to develop a comprehensive regulatory framework for AI systems, including those used in education. This includes classifying certain AI systems used in education as high-risk and implementing additional safeguards to protect against discrimination and violations of privacy and data protection laws. The EU also encourages interdisciplinary cooperation between AI developers and experts in areas such as non-discrimination, accessibility, and digital rights.\\n\\nOverall, while both the US and Europe recognize the potential benefits of AI technology in education, their approaches differ in terms of prioritizing research and development versus regulation and protection of individuals' rights.\"}\n",
            "Analyze the differences in positions taken by the two sources. \n",
            "Clearly state where they agree, where they disagree, and any important nuances in their stances. \n",
            "(Aim for about 200 words, in completed sentences.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nBoth the White House Executive Order and the European Union AI Act recognize the importance of education in the development and use of AI technology. They both acknowledge the potential benefits of AI in education, such as promoting innovation and addressing societal issues. However, they differ in their approaches to achieving these goals.\n\nThe White House Executive Order takes a more proactive stance, with a focus on promoting AI education and training in schools and universities, as well as investing in research and development. This approach prioritizes the advancement of AI technology and its potential impact on the economy and global competitiveness. In contrast, the European Union AI Act takes a more cautious approach, with a focus on regulating and managing the risks of AI, including potential job displacement and discrimination. This approach prioritizes protecting individuals' rights and privacy.\n\nOne key difference between the two sources is their emphasis on regulation. While the US approach focuses on promoting innovation and competitiveness, the EU approach prioritizes protecting citizens' rights and privacy. This is reflected in the EU's efforts to develop a comprehensive regulatory framework for AI systems, including those used in education. The EU also classifies certain AI systems used in education as high-risk and implements additional safeguards to protect against discrimination and violations of privacy and data protection laws.\n\nAnother difference is the level of interdisciplinary cooperation encouraged by the EU. The EU emphasizes the importance of collaboration between AI developers and experts in areas such as non-discrimination, accessibility, and digital rights. This highlights the EU's focus on addressing potential ethical and societal implications"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrHc19O--zCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"Compare regulations on CBRN weapons issues in AI\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "Y2U5g6Up89KE",
        "outputId": "c35282b3-472c-4a30-98d1-10fff11f0bc2"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'Compare regulations on CBRN weapons issues in AI':\n",
            "* White House Executive Order, November 2023: {'query': 'Compare regulations on CBRN weapons issues in AI', 'result': ' The regulations on CBRN weapons issues in AI include evaluating the potential for AI to be misused to enable the development or production of CBRN threats, while also considering the benefits and application of AI to counter these threats. Additionally, there are requirements for regulating or overseeing the training, deployment, publication, or use of AI models that may present CBRN risks to the United States, and recommendations for minimizing the risks of AI model misuse. There are also guidelines for performing security reviews and managing potential security risks of releasing federal data that could aid in the development of CBRN weapons.'} \n",
            "* European Union AI Act, March 2024: {'query': 'Compare regulations on CBRN weapons issues in AI', 'result': \" I'm sorry, I don't see any mention of CBRN weapons in the context provided. Can you provide more information or clarify your question?\"}\n",
            "Analyze the differences in positions taken by the two sources. \n",
            "Clearly state where they agree, where they disagree, and any important nuances in their stances. \n",
            "(Aim for about 200 words, in completed sentences.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nBoth the White House Executive Order and the European Union AI Act address the issue of regulating CBRN weapons in the context of AI. However, there are some key differences in their positions.\n\nFirstly, both sources agree that there is a need to evaluate the potential for AI to be misused in the development or production of CBRN weapons. They also both recognize the potential benefits of AI in countering these threats. This shows a shared understanding of the potential risks and benefits of AI in the context of CBRN weapons.\n\nHowever, the White House Executive Order takes a more proactive approach by requiring regulation and oversight of the training, deployment, publication, and use of AI models that may present CBRN risks. This indicates a stronger stance on the need for strict control and monitoring of AI in order to prevent its misuse in the development of CBRN weapons.\n\nOn the other hand, the European Union AI Act does not specifically mention CBRN weapons in its context. This could suggest a less urgent or comprehensive approach to regulating AI in the context of CBRN weapons. However, it is important to note that the Act does address the broader issue of AI regulation and ethics, which could indirectly impact the development and use of AI in relation to CBRN weapons.\n\nOverall, while both sources recognize the potential risks and benefits of AI in the context of CBRN weapons, the White House Executive Order takes a more specific and proactive stance on regulating and monitoring AI, while the"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run_agent(\"Compare regulations on synthetic nucleic acid issues in AI\")\n",
        "run_agent(\"Compare regulations on CBRN weapons issues in AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "1Y1hXnm8EgvW",
        "outputId": "9cad05da-e683-4f43-d5c7-17ac6e2f8923"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'Compare regulations on CBRN weapons issues in AI':\n",
            "* White House Executive Order, November 2023: {'query': 'Compare regulations on CBRN weapons issues in AI', 'result': ' The regulations on CBRN weapons issues in AI include evaluating the potential for AI to be misused to enable the development or production of CBRN threats, while also considering the benefits and application of AI to counter these threats. Additionally, there are requirements for regulating or overseeing the training, deployment, publication, or use of AI models that may present CBRN risks to the United States, and recommendations for minimizing the risks of AI model misuse. There are also guidelines for performing security reviews and managing potential security risks of releasing federal data that could aid in the development of CBRN weapons.'} \n",
            "* European Union AI Act, March 2024: {'query': 'Compare regulations on CBRN weapons issues in AI', 'result': \" I'm sorry, I don't see any mention of CBRN weapons in the context provided. Can you provide more information or clarify your question?\"}\n",
            "Analyze the differences in positions taken by the two sources. \n",
            "Clearly state where they agree, where they disagree, and any important nuances in their stances. (Aim for about 200 words.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBoth the White House Executive Order and the European Union AI Act address the issue of regulating CBRN weapons in the context of AI. However, there are some key differences in their approaches and stances.\\n\\nOne area of agreement is the recognition of the potential for AI to be misused in the development or production of CBRN weapons. Both sources acknowledge the need for regulations to address this risk and mitigate it. They also both emphasize the importance of considering the benefits and applications of AI in countering CBRN threats.\\n\\nHowever, there are also notable differences in their positions. The White House Executive Order takes a more proactive approach, with specific requirements for regulating the training, deployment, publication, and use of AI models that may pose CBRN risks. It also includes recommendations for minimizing the risks of AI model misuse. On the other hand, the European Union AI Act does not mention CBRN weapons specifically and does not have any specific regulations or requirements related to them. This could suggest a less urgent or comprehensive approach to addressing the issue.\\n\\nAnother difference is in the scope of their regulations. The White House Executive Order specifically mentions federal data that could aid in the development of CBRN weapons, while the European Union AI Act does not address this aspect. This could indicate a more comprehensive approach by the US government in regulating potential risks related to CBRN weapons and AI.\\n\\nOverall, while both sources recognize the potential risks and need for regulations in the context of CBRN weapons and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_agent(\"Compare regulations on CBRN weapons issues in AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "bIAA1yyY6w5e",
        "outputId": "4601f17f-c92b-4653-bf2b-586e621e3f93"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'Compare regulations on CBRN weapons issues in AI':\n",
            "* White House Executive Order, November 2023: {'query': 'Compare regulations on CBRN weapons issues in AI', 'result': ' The regulations on CBRN weapons issues in AI include evaluating the potential for AI to be misused to enable the development or production of CBRN threats, while also considering the benefits and application of AI to counter these threats. Additionally, there are requirements for regulating or overseeing the training, deployment, publication, or use of AI models that may present CBRN risks to the United States, and recommendations for minimizing the risks of AI model misuse. There are also guidelines for performing security reviews and managing potential security risks of releasing federal data that could aid in the development of CBRN weapons.'} \n",
            "* European Union AI Act, March 2024: {'query': 'Compare regulations on CBRN weapons issues in AI', 'result': \" I'm sorry, I don't see any mention of CBRN weapons in the context provided. Can you provide more information or clarify your question?\"}\n",
            "Analyze the differences in positions taken by the two sources. \n",
            "Clearly state where they agree, where they disagree, and any important nuances in their stances. (Aim for about 200 words.)\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBoth the White House Executive Order and the European Union AI Act address the issue of regulating CBRN weapons in the context of AI. However, there are some key differences in their approaches and stances.\\n\\nOne area of agreement is the recognition of the potential for AI to be misused in the development or production of CBRN weapons. Both sources acknowledge the need for regulations to address this risk and mitigate it. They also both emphasize the importance of considering the benefits and applications of AI in countering CBRN threats.\\n\\nHowever, there are also notable differences in their positions. The White House Executive Order takes a more proactive approach, with specific requirements for regulating the training, deployment, publication, and use of AI models that may pose CBRN risks. It also includes recommendations for minimizing the risks of AI model misuse. On the other hand, the European Union AI Act does not mention CBRN weapons specifically and does not have any specific regulations or requirements related to them. This could suggest a less urgent or comprehensive approach to addressing the issue.\\n\\nAnother difference is in the scope of their regulations. The White House Executive Order specifically mentions federal data that could aid in the development of CBRN weapons, while the European Union AI Act does not address this aspect. This could indicate a more targeted focus on potential security risks in the US, while the EU Act may have a broader scope.\\n\\nOverall, both sources agree on the need for regulations on CBRN weapons in the context of AI,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_agent(\"Compare regulations on biometric data in AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "EaIWppo-QtTv",
        "outputId": "dfec06a3-ffdf-4677-9b95-f80437b83475"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBoth the White House Executive Order and the European Union AI Act address the use of biometric data in AI and aim to protect individuals from potential harm and discrimination. They both recognize the importance of privacy and the need for transparency when using biometric data in AI systems.\\n\\nHowever, there are some differences in their positions. The White House Executive Order does not specifically mention regulations on biometric data in AI, but rather focuses on the need to protect people with disabilities from unequal treatment and the use of privacy-enhancing technologies. On the other hand, the European Union AI Act has specific regulations in place, such as prohibiting the use of biometric data to infer sensitive personal information and restricting its use for remote identification and social scoring.\\n\\nAnother difference is that the White House Executive Order is from November 2023, while the European Union AI Act is from March 2024. This could indicate that the European Union may have more advanced and specific regulations on biometric data in AI compared to the United States.\\n\\nOverall, both sources agree on the importance of protecting individuals from potential harm and discrimination in the use of biometric data in AI. However, the European Union AI Act has more specific regulations in place compared to the White House Executive Order, which focuses more broadly on the use of privacy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_agent(\"Compare regulations on copyright and creative works in AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "wmjLqET81DY7",
        "outputId": "ae893680-a69a-4b54-be2b-02877f8275cc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou have the following responses on the issue of 'Compare regulations on copyright and creative works in AI':\n",
            "* White House Executive Order, November 2023: {'query': 'Compare regulations on copyright and creative works in AI', 'result': ' The regulations on copyright and creative works in AI are addressed in section (c) of the Executive Order. This section requires the Under Secretary of Commerce for Intellectual Property and Director of the United States Patent and Trademark Office (USPTO Director) to publish guidance on inventorship and the use of AI in the inventive process. It also requires the USPTO Director to consult with the Director of the United States Copyright Office and issue recommendations on potential executive actions relating to copyright and AI. Additionally, the Secretary of Homeland Security is tasked with developing a training, analysis, and evaluation program to mitigate AI-related IP risks. These regulations aim to address issues of inventorship and copyright protection in works produced using AI, as well as potential discrimination and bias in the use of AI in creative processes.'} \n",
            "* European Union AI Act, March 2024: {'query': 'Compare regulations on copyright and creative works in AI', 'result': ' The compromise agreement states that providers of GPAI models will need to put in place a policy to respect Union copyright law, as well as make publicly available a sufficiently detailed summary about the content used for training of the general-purpose AI model. This summary should not be technically detailed but comprehensive at a general level, while taking into due account the need to protect trade secrets and confidential business information. This is to increase transparency on the data used in the training of AI models and to ensure that providers comply with Union law on copyright and related rights. The AI Office will monitor compliance with these obligations, but will not conduct a work-by-work assessment of the training data for copyright compliance. Additionally, the compromise agreement states that compliance with these obligations should be commensurate and proportionate to the type of model provider. This means that different regulations may apply to different types of providers, depending on their role in the development and deployment of AI systems.'}\n",
            "Analyze the differences in positions taken by the two sources. \n",
            "Clearly state where they agree, where they disagree, and any important nuances in their stances. If you have them, refer to specific page numbers\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBoth the White House Executive Order and the European Union AI Act address the issue of copyright and creative works in AI. They both recognize the need for regulations to address potential issues such as inventorship, copyright protection, and discrimination in the use of AI in creative processes.\\n\\nOne key difference between the two sources is the approach to addressing these issues. The White House Executive Order focuses on guidance and recommendations, while the European Union AI Act includes specific requirements and obligations for providers of general-purpose AI models. The Executive Order tasks the USPTO Director with publishing guidance and consulting with the Director of the United States Copyright Office, while the AI Act requires providers to have a policy in place to respect Union copyright law and make a summary of the training data publicly available.\\n\\nAnother difference is the level of detail and specificity in the regulations. The Executive Order does not provide specific details on how to address these issues, while the AI Act includes specific requirements for providers and the AI Office to monitor compliance. The AI Act also takes into account the need to protect trade secrets and confidential business information, while the Executive Order does not mention this.\\n\\nHowever, both sources agree on the importance of addressing these issues and recognize the potential risks and challenges posed by AI in the creative industry. They also both acknowledge the need'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_agent(\"Compare regulations on copyright and creative works in AI\")\n",
        "display(Markdown( response ))"
      ],
      "metadata": {
        "id": "GF9K0cZB3koh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "r5TJV_PuBk1O",
        "outputId": "8332d6d7-9947-4368-8a52-064f16e5f74c"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Both the White House Executive Order and the European Union AI Act address the issue of copyright and creative works in AI. They both recognize the need for regulations to address potential issues such as inventorship, copyright protection, and discrimination in the use of AI in creative processes.\n\nOne key difference between the two sources is the approach to addressing these issues. The White House Executive Order focuses on guidance and recommendations, while the European Union AI Act includes specific requirements and obligations for providers of general-purpose AI models. The Executive Order tasks the USPTO Director with publishing guidance and consulting with the Director of the United States Copyright Office, while the AI Act requires providers to have a policy in place to respect Union copyright law and make a summary of the training data publicly available.\n\nAnother difference is the level of detail and specificity in the regulations. The Executive Order does not provide specific details on how to address these issues, while the AI Act includes specific requirements for providers and the AI Office to monitor compliance. The AI Act also takes into account the need to protect trade secrets and confidential business information, while the Executive Order does not mention this.\n\nHowever, both sources agree on the importance of addressing these issues and recognize the potential risks and challenges posed by AI in the creative industry. They also both acknowledge the need"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flA7_YSeBoMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}